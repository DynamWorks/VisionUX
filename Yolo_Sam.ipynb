{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/thebird/Dynamworks/LLM_Module/Hackathon/models/sam_vit_h_4b8939.pth ; exist: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"models\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv # pip install supervision==0.24.0\n",
    "\n",
    "image_bgr = cv2.imread(\"/home/thebird/Dynamworks/LLM_Module/Hackathon/Vehicle_Detection_Image_Dataset/sample_image.jpg\")\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "masks = mask_generator.generate(image_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(masks))\n",
    "print(masks[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator(color_lookup = sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=masks)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "select_mask  = [mask[\"segmentation\"] for mask in masks[0:25]]\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=select_mask,\n",
    "    grid_size= (5,5),\n",
    "    size=(20,20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "from IPython.display import display, Image\n",
    "\n",
    "model = YOLO('/home/thebird/Dynamworks/LLM_Module/Hackathon/runs/detect/train/weights/best.pt')\n",
    "model.fuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Vehicle'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLASS_NAMES_DICT = model.model.names\n",
    "\n",
    "# class_ids of interest - based on the number of classses\n",
    "CLASS_ID = [item for item in range(0,len(CLASS_NAMES_DICT))]\n",
    "\n",
    "CLASS_NAMES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_dimensions(cap):\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    return width, height\n",
    "\n",
    "def add_color_to_mask(mask, color):\n",
    "    # Convert the color tensor to CPU\n",
    "    color = torch.tensor(color).cpu().numpy()\n",
    "\n",
    "    # Create a binary mask based on the original mask\n",
    "    color_mask = np.zeros_like(mask.cpu().numpy(), dtype=np.uint8)\n",
    "    color_mask[mask.cpu().numpy() > 0] = 1  # Set non-zero values to 1\n",
    "\n",
    "    # Expand the color tensor and apply it to the binary mask\n",
    "    colored_mask = color_mask[..., None] * color\n",
    "\n",
    "    return colored_mask\n",
    "\n",
    "def draw_class_names(frame, class_names, positions, color, font_size=0.5):\n",
    "    for class_name, position in zip(class_names, positions):\n",
    "        cv2.putText(frame, class_name, position, cv2.FONT_HERSHEY_SIMPLEX, font_size, color, 2, cv2.LINE_AA)\n",
    "\n",
    "def draw_yolov8_boxes(frame, boxes, color):\n",
    "    for box in boxes:\n",
    "        box = list(map(int, box))\n",
    "        cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
    "\n",
    "def get_predicted_masks(model, mask_predictor, frame, width, height):\n",
    "    # Run frame through YOLOv8 to get detections\n",
    "    detections = model.predict(frame, conf=0.7)\n",
    "\n",
    "    # Check if there are fish detections\n",
    "    if len(detections[0].boxes) == 0:\n",
    "        return None, None, None, None  # Skip processing for frames without fish detections\n",
    "\n",
    "    # Run frame and detections through SAM to get masks\n",
    "    transformed_boxes = mask_predictor.transform.apply_boxes_torch(\n",
    "        detections[0].boxes.xyxy, [width, height]\n",
    "    )\n",
    "    mask_predictor.set_image(frame)\n",
    "    masks, scores, logits = mask_predictor.predict_torch(\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "        point_coords=None,\n",
    "        point_labels=None\n",
    "    )\n",
    "    return masks, scores, logits, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x44495658/'XVID' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 3 Vehicles, 61.3ms\n",
      "Speed: 3.3ms preprocess, 61.3ms inference, 86.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip processing for empty frames\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m masks, scores, logits, detections \u001b[38;5;241m=\u001b[39m \u001b[43mget_predicted_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mmask_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(detections[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip processing for frames without fish detections\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mget_predicted_masks\u001b[0;34m(model, mask_predictor, frame, width, height)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Run frame and detections through SAM to get masks\u001b[39;00m\n\u001b[1;32m     37\u001b[0m transformed_boxes \u001b[38;5;241m=\u001b[39m mask_predictor\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mapply_boxes_torch(\n\u001b[1;32m     38\u001b[0m     detections[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy, [width, height]\n\u001b[1;32m     39\u001b[0m )\n\u001b[0;32m---> 40\u001b[0m \u001b[43mmask_predictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m masks, scores, logits \u001b[38;5;241m=\u001b[39m mask_predictor\u001b[38;5;241m.\u001b[39mpredict_torch(\n\u001b[1;32m     42\u001b[0m     boxes\u001b[38;5;241m=\u001b[39mtransformed_boxes,\n\u001b[1;32m     43\u001b[0m     multimask_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     44\u001b[0m     point_coords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     45\u001b[0m     point_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masks, scores, logits, detections\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/predictor.py:60\u001b[0m, in \u001b[0;36mSamPredictor.set_image\u001b[0;34m(self, image, image_format)\u001b[0m\n\u001b[1;32m     57\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(input_image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     58\u001b[0m input_image_torch \u001b[38;5;241m=\u001b[39m input_image_torch\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_torch_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image_torch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/predictor.py:89\u001b[0m, in \u001b[0;36mSamPredictor.set_torch_image\u001b[0;34m(self, transformed_image, original_image_size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(transformed_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m     88\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpreprocess(transformed_image)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_image_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:112\u001b[0m, in \u001b[0;36mImageEncoderViT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m--> 112\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneck(x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:174\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m     H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    172\u001b[0m     x, pad_hw \u001b[38;5;241m=\u001b[39m window_partition(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size)\n\u001b[0;32m--> 174\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Reverse window partition\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:234\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m attn \u001b[38;5;241m=\u001b[39m (q \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale) \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_rel_pos:\n\u001b[0;32m--> 234\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[43madd_decomposed_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrel_pos_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39msoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    237\u001b[0m x \u001b[38;5;241m=\u001b[39m (attn \u001b[38;5;241m@\u001b[39m v)\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, H, W, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/modeling/image_encoder.py:349\u001b[0m, in \u001b[0;36madd_decomposed_rel_pos\u001b[0;34m(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)\u001b[0m\n\u001b[1;32m    347\u001b[0m q_h, q_w \u001b[38;5;241m=\u001b[39m q_size\n\u001b[1;32m    348\u001b[0m k_h, k_w \u001b[38;5;241m=\u001b[39m k_size\n\u001b[0;32m--> 349\u001b[0m Rh \u001b[38;5;241m=\u001b[39m \u001b[43mget_rel_pos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_pos_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m Rw \u001b[38;5;241m=\u001b[39m get_rel_pos(q_w, k_w, rel_pos_w)\n\u001b[1;32m    352\u001b[0m B, _, dim \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Replace the following line with your actual VIDEO_PATH\n",
    "VIDEO_PATH = \"/home/thebird/Dynamworks/LLM_Module/Hackathon/Vehicle_Detection_Image_Dataset/sample_video.mp4\"\n",
    "OUTPUT_VIDEO_PATH = \"/home/thebird/Dynamworks/LLM_Module/Hackathon/working/sample_video_yolo_sam.mp4\"\n",
    "\n",
    "# This will contain the resulting mask predictions for local use\n",
    "mask_frames = []\n",
    "\n",
    "\n",
    "constant_mask_color = np.array([0, 0, 255], dtype=np.uint8)  # Red color for masks\n",
    "output_class_color = (0, 255, 0)  # Green color for class names\n",
    "yolov8_box_color = (255, 0, 0)  # Blue color for YOLOv8 bounding boxes\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_video = cv2.VideoWriter(OUTPUT_VIDEO_PATH, fourcc, 15.0, (width, height))\n",
    "\n",
    "frame_num = 1\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Check if the frame is empty or None\n",
    "    if frame is None:\n",
    "        continue  # Skip processing for empty frames\n",
    "\n",
    "    masks, scores, logits, detections = get_predicted_masks(model,\n",
    "                                                            mask_predictor,\n",
    "                                                            frame,\n",
    "                                                            width,\n",
    "                                                            height)\n",
    "\n",
    "    if len(detections[0].boxes) == 0:\n",
    "        continue  # Skip processing for frames without fish detections\n",
    "\n",
    "    # Check if the mask is empty\n",
    "    if masks[0][0].numel() == 0:\n",
    "        continue  # Skip processing for empty masks\n",
    "\n",
    "    # Combine mask predictions into a single mask, each with the same color\n",
    "    class_ids = detections[0].boxes.cpu().cls\n",
    "    merged_with_colors = add_color_to_mask(masks[0][0], constant_mask_color)\n",
    "    for i in range(1, len(masks)):\n",
    "        curr_mask_with_colors = add_color_to_mask(masks[i][0], constant_mask_color)\n",
    "        merged_with_colors = np.bitwise_or(merged_with_colors, curr_mask_with_colors)\n",
    "\n",
    "    # Draw YOLOv8 bounding boxes on the frame\n",
    "    draw_yolov8_boxes(frame, detections[0].boxes.xyxy, yolov8_box_color)\n",
    "\n",
    "    # Draw class names on the frame with a slightly larger font\n",
    "    class_names = [CLASS_NAMES_DICT[int(class_id)] for class_id in class_ids]\n",
    "    draw_class_names(frame, class_names, [(int(box[0]), int(box[1])) for box in detections[0].boxes.xyxy], output_class_color, font_size=0.7)\n",
    "\n",
    "    # Overlay the SAM masks onto the frame\n",
    "    frame_with_masks = cv2.addWeighted(frame, 1, merged_with_colors, 0.5, 0)\n",
    "\n",
    "    # Write the frame with masks, YOLOv8 boxes, and class names to the output video\n",
    "    output_video.write(frame_with_masks)\n",
    "\n",
    "    frame_num += 1\n",
    "\n",
    "cap.release()\n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Final\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import rerun as rr  # pip install rerun-sdk\n",
    "import rerun.blueprint as rrb\n",
    "import torch\n",
    "import torchvision\n",
    "from segment_anything import SamAutomaticMaskGenerator, sam_model_registry\n",
    "from segment_anything.modeling import Sam\n",
    "from tqdm import tqdm\n",
    "\n",
    "DESCRIPTION = \"\"\"\n",
    "Example of using Rerun to log and visualize the output of [Segment Anything](https://segment-anything.com/).\n",
    "\n",
    "The full source code for this example is available [on GitHub](https://github.com/rerun-io/rerun/blob/latest/examples/python/segment_anything_model).\n",
    "\"\"\".strip()\n",
    "\n",
    "MODEL_DIR: Final = os.path.join(os.getcwd(),\"models/\")\n",
    "MODEL_URLS: Final = {\n",
    "    \"vit_h\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
    "    \"vit_l\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n",
    "    \"vit_b\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sam(model: str, device: str, model_name: str) -> Sam:\n",
    "    \"\"\"Load the segment-anything model, fetching the model-file as necessary.\"\"\"\n",
    "    model_path = os.path.join(MODEL_DIR, model_name)\n",
    "\n",
    "    logging.info(f\"PyTorch version: {torch.__version__}\")\n",
    "    logging.info(f\"Torchvision version: {torchvision.__version__}\")\n",
    "    logging.info(f\"CUDA is available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    logging.info(f\"Building sam from: {model_path}\")\n",
    "    sam = sam_model_registry[model](checkpoint=model_path)\n",
    "    return sam.to(device=device)\n",
    "\n",
    "def run_segmentation(mask_generator: SamAutomaticMaskGenerator, image: cv2.typing.MatLike) -> None:\n",
    "    \"\"\"Run segmentation on a single image.\"\"\"\n",
    "    rr.log(\"image\", rr.Image(image))\n",
    "\n",
    "    logging.info(\"Finding masks\")\n",
    "    masks = mask_generator.generate(image)\n",
    "\n",
    "    logging.info(f\"Found {len(masks)} masks\")\n",
    "\n",
    "    # Log all the masks stacked together as a tensor\n",
    "    # TODO(jleibs): Tensors with class-ids and annotation-coloring would make this much slicker\n",
    "    mask_tensor = (\n",
    "        np.dstack([np.zeros((image.shape[0], image.shape[1]))] + [m[\"segmentation\"] for m in masks]).astype(\"uint8\")\n",
    "        * 128\n",
    "    )\n",
    "    rr.log(\"mask_tensor\", rr.Tensor(mask_tensor))\n",
    "\n",
    "    # Note: for stacking, it is important to sort these masks by area from largest to smallest\n",
    "    # this is because the masks are overlapping and we want smaller masks to\n",
    "    # be drawn on top of larger masks.\n",
    "    # TODO(jleibs): we could instead draw each mask as a separate image layer, but the current layer-stacking\n",
    "    # does not produce great results.\n",
    "    masks_with_ids = list(enumerate(masks, start=1))\n",
    "    print(masks_with_ids[0][1])\n",
    "    masks_with_ids.sort(key=(lambda x: x[1][\"area\"]), reverse=True)  # type: ignore[no-any-return]\n",
    "\n",
    "    # Layer all of the masks together, using the id as class-id in the segmentation\n",
    "    segmentation_img = np.zeros((image.shape[0], image.shape[1]))\n",
    "    for id, m in masks_with_ids:\n",
    "        segmentation_img[m[\"segmentation\"]] = id\n",
    "\n",
    "    rr.log(\"image/masks\", rr.SegmentationImage(segmentation_img.astype(np.uint8)))\n",
    "\n",
    "    mask_bbox = np.array([m[\"bbox\"] for _, m in masks_with_ids])\n",
    "    rr.log(\n",
    "        \"image/boxes\",\n",
    "        rr.Boxes2D(array=mask_bbox, array_format=rr.Box2DFormat.XYWH, class_ids=[id for id, _ in masks_with_ids]),\n",
    "    )\n",
    "\n",
    "def load_image(image) -> cv2.typing.MatLike:\n",
    "    \"\"\"Conditionally download an image from URL or load it from disk.\"\"\"\n",
    "    logging.info(f\"Loading: image\")\n",
    "\n",
    "    # Rerun can handle BGR as well, but SAM requires RGB.\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(\n",
    "#         description=\"Run the Facebook Research Segment Anything example.\",\n",
    "#         formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "#     )\n",
    "\n",
    "# rr.script_add_args(parser)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\n",
    "blueprint = rrb.Vertical(\n",
    "        rrb.Spatial2DView(name=\"Image and segmentation mask\", origin=\"/image\"),\n",
    "        rrb.Horizontal(\n",
    "            rrb.TextLogView(name=\"Log\", origin=\"/logs\"),\n",
    "            rrb.TextDocumentView(name=\"Description\", origin=\"/description\"),\n",
    "            column_shares=[2, 1],\n",
    "        ),\n",
    "        row_shares=[3, 1],\n",
    "    )\n",
    "\n",
    "# rr.script_setup(args, \"rerun_example_segment_anything_model\", default_blueprint=blueprint)\n",
    "logging.getLogger().addHandler(rr.LoggingHandler(\"logs\"))\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "rr.log(\"description\", rr.TextDocument(DESCRIPTION, media_type=rr.MediaType.MARKDOWN), timeless=True)\n",
    "\n",
    "sam = create_sam(\"vit_h\", \"cuda\", \"sam_vit_h_4b8939.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_config = {\"points_per_batch\": 32}\n",
    "mask_generator = SamAutomaticMaskGenerator(sam, **mask_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"/home/thebird/Dynamworks/LLM_Module/Hackathon/working/sample_video.mp4\")\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "images_in_video = []\n",
    "frame_num = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if frame_num > 8:\n",
    "        images_in_video.append(frame)\n",
    "    frame_num += 1\n",
    "    # if frame_num == 10:\n",
    "        # break\n",
    "\n",
    "cap.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168925, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0428577661514282, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.9829621911048889, 'crop_box': [0, 0, 1280, 720]}\n"
     ]
    }
   ],
   "source": [
    "rr.set_time_sequence(\"image\", 0)\n",
    "image = load_image(images_in_video[0])\n",
    "run_segmentation(mask_generator, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168925, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0428577661514282, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.9829621911048889, 'crop_box': [0, 0, 1280, 720]}\n",
      "0\n",
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168913, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0433796644210815, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.9827965497970581, 'crop_box': [0, 0, 1280, 720]}\n",
      "1\n",
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168886, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0442249774932861, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.9826681017875671, 'crop_box': [0, 0, 1280, 720]}\n",
      "2\n",
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168937, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0422040224075317, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.9822144508361816, 'crop_box': [0, 0, 1280, 720]}\n",
      "3\n",
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168880, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0441536903381348, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.982991635799408, 'crop_box': [0, 0, 1280, 720]}\n",
      "4\n",
      "{'segmentation': array([[ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       [ True,  True,  True, ...,  True,  True,  True],\n",
      "       ...,\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False],\n",
      "       [False, False, False, ..., False, False, False]]), 'area': 168894, 'bbox': [0, 0, 1279, 197], 'predicted_iou': 1.0432265996932983, 'point_coords': [[180.0, 101.25]], 'stability_score': 0.9829070568084717, 'crop_box': [0, 0, 1280, 720]}\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m rr\u001b[38;5;241m.\u001b[39mset_time_sequence(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, n)\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m load_image(image_uri)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mrun_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(n)\n",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m, in \u001b[0;36mrun_segmentation\u001b[0;34m(mask_generator, image)\u001b[0m\n\u001b[1;32m     15\u001b[0m rr\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m, rr\u001b[38;5;241m.\u001b[39mImage(image))\n\u001b[1;32m     17\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding masks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[43mmask_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(masks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m masks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Log all the masks stacked together as a tensor\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# TODO(jleibs): Tensors with class-ids and annotation-coloring would make this much slicker\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:163\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator.generate\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mGenerates masks for the given image.\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m         the mask, given in XYWH format.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Generate masks\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m mask_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# Filter small disconnected regions and holes in masks\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_mask_region_area \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:206\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._generate_masks\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    204\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m crop_box, layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(crop_boxes, layer_idxs):\n\u001b[0;32m--> 206\u001b[0m     crop_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(crop_data)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Remove duplicate masks between crops\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:245\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_crop\u001b[0;34m(self, image, crop_box, crop_layer_idx, orig_size)\u001b[0m\n\u001b[1;32m    243\u001b[0m data \u001b[38;5;241m=\u001b[39m MaskData()\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (points,) \u001b[38;5;129;01min\u001b[39;00m batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoints_per_batch, points_for_image):\n\u001b[0;32m--> 245\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcropped_im_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_box\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m     data\u001b[38;5;241m.\u001b[39mcat(batch_data)\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m batch_data\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/automatic_mask_generator.py:297\u001b[0m, in \u001b[0;36mSamAutomaticMaskGenerator._process_batch\u001b[0;34m(self, points, im_size, crop_box, orig_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_iou_thresh \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    296\u001b[0m     keep_mask \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou_preds\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_iou_thresh\n\u001b[0;32m--> 297\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Calculate stability score\u001b[39;00m\n\u001b[1;32m    300\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstability_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m calculate_stability_score(\n\u001b[1;32m    301\u001b[0m     data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmask_threshold, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstability_score_offset\n\u001b[1;32m    302\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/dynamworks_hackathon/lib/python3.10/site-packages/segment_anything/utils/amg.py:49\u001b[0m, in \u001b[0;36mMaskData.filter\u001b[0;34m(self, keep)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[k] \u001b[38;5;241m=\u001b[39m v[torch\u001b[38;5;241m.\u001b[39mas_tensor(keep, device\u001b[38;5;241m=\u001b[39mv\u001b[38;5;241m.\u001b[39mdevice)]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stats[k] \u001b[38;5;241m=\u001b[39m v[keep\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n, image_uri in enumerate(images_in_video):\n",
    "    rr.set_time_sequence(\"image\", n)\n",
    "    image = load_image(image_uri)\n",
    "    run_segmentation(mask_generator, image)\n",
    "    print(n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamworks_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
