{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warnings in the notebook to maintain clean output cells\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import yaml\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the visual appearance of Seaborn plots\n",
    "sns.set(rc={'axes.facecolor': '#eae8fa'}, style='darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8n model from Ultralytics\n",
    "model = YOLO('yolov8n.pt')\n",
    "# this includes car and truck classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.path.abspath('')\n",
    "unseen_image_path = os.path.join(dirname, 'Vehicle_Detection_Image_Dataset/sample_image.jpg')\n",
    "\n",
    "# Perform inference on the provided image(s)\n",
    "results = model.predict(source=unseen_image_path, \n",
    "                    imgsz=640,  # Resize image to 640x640 (the size pf images the model was trained on)\n",
    "                    conf=0.5)   # Confidence threshold: 50% (only detections above 50% confidence will be considered)\n",
    "\n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "# plt.figure(figsize=(20,15))\n",
    "# plt.imshow(sample_image)\n",
    "# plt.title('Detected Objects in Sample Image by the Pre-trained YOLOv8 Model on COCO Dataset', fontsize=20)\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(dirname, 'Vehicle_Detection_Image_Dataset/')\n",
    "\n",
    "# Set the path to the YAML file\n",
    "yaml_file_path = os.path.join(dataset_path, 'data.yaml')\n",
    "\n",
    "# Load and print the contents of the YAML file\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    yaml_content = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    print(yaml.dump(yaml_content, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for training and validation image sets\n",
    "train_images_path = os.path.join(dataset_path, 'train', 'images')\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# Initialize counters for the number of images\n",
    "num_train_images = 0\n",
    "num_valid_images = 0\n",
    "\n",
    "# Initialize sets to hold the unique sizes of images\n",
    "train_image_sizes = set()\n",
    "valid_image_sizes = set()\n",
    "\n",
    "# Check train images sizes and count\n",
    "for filename in os.listdir(train_images_path):\n",
    "    if filename.endswith('.jpg'):  \n",
    "        num_train_images += 1\n",
    "        image_path = os.path.join(train_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            train_image_sizes.add(img.size)\n",
    "\n",
    "# Check validation images sizes and count\n",
    "for filename in os.listdir(valid_images_path):\n",
    "    if filename.endswith('.jpg'): \n",
    "        num_valid_images += 1\n",
    "        image_path = os.path.join(valid_images_path, filename)\n",
    "        with Image.open(image_path) as img:\n",
    "            valid_image_sizes.add(img.size)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Number of training images: {num_train_images}\")\n",
    "print(f\"Number of validation images: {num_valid_images}\")\n",
    "\n",
    "# Check if all images in training set have the same size\n",
    "if len(train_image_sizes) == 1:\n",
    "    print(f\"All training images have the same size: {train_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Training images have varying sizes.\")\n",
    "\n",
    "# Check if all images in validation set have the same size\n",
    "if len(valid_image_sizes) == 1:\n",
    "    print(f\"All validation images have the same size: {valid_image_sizes.pop()}\")\n",
    "else:\n",
    "    print(\"Validation images have varying sizes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(train_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 8 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 8)]\n",
    "\n",
    "# Create a 2x4 subplot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 11))\n",
    "\n",
    "# Display each of the selected images\n",
    "for ax, img_file in zip(axes.ravel(), selected_images):\n",
    "    img_path = os.path.join(train_images_path, img_file)\n",
    "    image = Image.open(img_path)\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')  \n",
    "\n",
    "plt.suptitle('Sample Images from Training Dataset', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on our custom dataset\n",
    "results = model.train(\n",
    "    data=yaml_file_path,     # Path to the dataset configuration file\n",
    "    epochs=100,              # Number of epochs to train for\n",
    "    imgsz=640,               # Size of input images as integer\n",
    "    device=0,                # Device to run on, i.e. cuda device=0 \n",
    "    patience=50,             # Epochs to wait for no observable improvement for early stopping of training\n",
    "    batch=32,                # Number of images per batch\n",
    "    optimizer='auto',        # Optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
    "    lr0=0.0001,              # Initial learning rate \n",
    "    lrf=0.1,                 # Final learning rate (lr0 * lrf)\n",
    "    dropout=0.1,             # Use dropout regularization\n",
    "    seed=0                   # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_training_files_path = os.path.join(dirname, 'runs/detect/train')\n",
    "\n",
    "# Construct the path to the best model weights file using os.path.join\n",
    "best_model_path = os.path.join(post_training_files_path, 'weights/best.pt')\n",
    "\n",
    "# Load the best model weights into the YOLO model\n",
    "best_model = YOLO(best_model_path, results)\n",
    "\n",
    "# Validate the best model using the validation set with default parameters\n",
    "metrics = best_model.val(split='val')\n",
    "\n",
    "\n",
    "# Define the path to the validation images\n",
    "valid_images_path = os.path.join(dataset_path, 'valid', 'images')\n",
    "\n",
    "# List all jpg images in the directory\n",
    "image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]\n",
    "\n",
    "# Select 9 images at equal intervals\n",
    "num_images = len(image_files)\n",
    "selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]\n",
    "\n",
    "# Initialize the subplot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 21))\n",
    "fig.suptitle('Validation Set Inferences', fontsize=24)\n",
    "\n",
    "# Perform inference on each selected image and display it\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image_path = os.path.join(valid_images_path, selected_images[i])\n",
    "    results = best_model.predict(source=image_path, imgsz=640, conf=0.5, save_txt=True)\n",
    "    # add sam2 detection\n",
    "    # find a way to read the video frame by frame\n",
    "    # for each frame do a prediction, then add the sam2 mask\n",
    "    # save the annotated and masked image, then fuse the frames into a video\n",
    "    # save the masks(image) and see what LLMs can do with it\n",
    "    annotated_image = results[0].plot(line_width=1)\n",
    "    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(annotated_image_rgb)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference on the provided image using best model\n",
    "results = best_model.predict(source=unseen_image_path, imgsz=640, conf=0.7) \n",
    "                        \n",
    "# Annotate and convert image to numpy array\n",
    "sample_image = results[0].plot(line_width=2)\n",
    "csv_result = results[0].to_df()\n",
    "print(csv_result)\n",
    "# np.savetxt(\"csv_result.csv\", csv_result, delimiter=',')\n",
    "csv_result.to_csv('out.csv', index=False)  \n",
    "\n",
    "# Convert the color of the image from BGR to RGB for correct color representation in matplotlib\n",
    "sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Display annotated image\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.imshow(sample_image)\n",
    "plt.title('Detected Objects in Sample Image by the Fine-tuned YOLOv8 Model', fontsize=20)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the sample video in the dataset\n",
    "dataset_video_path = os.path.join(dirname, 'Vehicle_Detection_Image_Dataset/sample_video.mp4')\n",
    "\n",
    "# Define the destination path in the working directory\n",
    "video_path = os.path.join(dirname, 'working/sample_video.mp4')\n",
    "\n",
    "# Copy the video file from its original location in the dataset to the current working directory in Kaggle for further processing\n",
    "shutil.copyfile(dataset_video_path, video_path)\n",
    "\n",
    "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
    "test_results = best_model.predict(source=video_path, save=True, save_txt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "with open('data.json', 'w') as file:\n",
    "    json.dump(json.dumps(test_results[0].to_json(normalize=True)), file)\n",
    "\n",
    "for i, r in enumerate(test_results):\n",
    "    test_csv_result = r.to_df()\n",
    "    test_csv_result.to_csv(f'output_csv/data_{i}.csv', index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_files(folder_path):\n",
    "    # Get a list of all files in the folder\n",
    "    files = os.listdir(folder_path)\n",
    "    \n",
    "    # Filter out the CSV files\n",
    "    csv_files = sorted([file for file in files if file.endswith('.csv')])\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "# Example usage:\n",
    "f_p = 'output_csv'\n",
    "csv_files = get_csv_files(f_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv files from output_csv/\n",
    "# have a main dataframe\n",
    "main_df = pd.DataFrame()  # Main DataFrame to store all detections\n",
    "    \n",
    "for frame_count, csv_file in enumerate(csv_files, start=1):  # Start frame_count from 1\n",
    "    file_path = os.path.join('output_csv/', csv_file)\n",
    "        \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "        \n",
    "    # Add the frame column\n",
    "    df['frame'] = frame_count\n",
    "        \n",
    "    # Append to the main DataFrame\n",
    "    main_df = pd.concat([main_df, df], ignore_index=True)\n",
    "    \n",
    "# for each new file, increase the frame count column\n",
    "main_df.rename(columns={\"Unnamed: 0\": \"detection_id\"}, inplace=True)\n",
    "print(main_df.head())\n",
    "# frame, detection_id, name, class, confidence, box\n",
    "main_df.to_csv('main_df.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': f\"\"\"This is a dataframe of detections from a CCTV footage at a traffic signal. Each frame's size is 384x640 {main_df} \\n\n",
    "                    Could you interpret the number of vehicles detected between frames 1 and 5? Without using code and just simple math.\"\"\",\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i /home/thebird/Dynamworks/LLM_Module/Hackathon/runs/detect/predict/sample_video.avi /home/thebird/Dynamworks/LLM_Module/Hackathon/working/processed_sample_video.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"/home/thebird/Dynamworks/LLM_Module/Hackathon/working/processed_sample_video.mp4\", embed=True, width=960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "\n",
    "# # Define the folder containing the images and the output video file name\n",
    "# image_folder = os.path.join(dirname, 'AAU_RainSnow_Dataset/aaurainsnow/Hasserisvej/Hasserisvej-1/cam1.mkv')\n",
    "# output_video = os.path.join(dirname, 'AAU_RainSnow_Dataset/aaurainsnow/Hasserisvej/Hasserisvej-1/aau_output_video.mp4')\n",
    "\n",
    "# # Run the ffmpeg command to convert mkv to mp4\n",
    "# command = ['ffmpeg', '-i', image_folder, '-codec', 'copy', output_video]\n",
    "\n",
    "# # Execute the command\n",
    "# subprocess.run(command)\n",
    "\n",
    "# print(f'Conversion complete: {output_video}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the sample video in the dataset\n",
    "aau_dataset_video_path = os.path.join(dirname, 'AAU_RainSnow_Dataset/aaurainsnow/Hasserisvej/Hasserisvej-1/aau_output_video.mp4')\n",
    "\n",
    "# Define the destination path in the working directory\n",
    "aau_video_path = os.path.join(dirname, 'working/aau_sample_video.mp4')\n",
    "\n",
    "# Copy the video file from its original location in the dataset to the current working directory in Kaggle for further processing\n",
    "shutil.copyfile(aau_dataset_video_path, aau_video_path)\n",
    "\n",
    "# Initiate vehicle detection on the sample video using the best performing model and save the output\n",
    "best_model.predict(source=aau_video_path, save=True, save_txt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the .avi video generated by the YOLOv8 prediction to .mp4 format for compatibility with notebook display\n",
    "!ffmpeg -y -loglevel panic -i /home/thebird/Dynamworks/LLM_Module/Hackathon/runs/detect/predict2/aau_sample_video.avi /home/thebird/Dynamworks/LLM_Module/Hackathon/working/aau_processed_sample_video.mp4\n",
    "\n",
    "# Embed and display the processed sample video within the notebook\n",
    "Video(\"/home/thebird/Dynamworks/LLM_Module/Hackathon/working/aau_processed_sample_video.mp4\", embed=True, width=960)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dynamworks_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
